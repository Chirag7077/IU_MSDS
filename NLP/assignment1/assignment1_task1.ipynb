{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L665 ML for NLPSpring 2018 \n",
    "\n",
    "## Assignment 1 - Task 1 \n",
    "\n",
    "Author: Carlos Sathler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read toxic comments dataset and create train and test partitions\n",
    "\n",
    "Source: Kaggle Toxic Comment Classification Challenge (https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.read_csv('input/train.csv')\n",
    "drop_cols = ['id', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "df_all = df_all.drop(drop_cols, axis=1)\n",
    "#df_all = df_all.sample(frac=0.2)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of toxic comments: 0.09584448302009764\n"
     ]
    }
   ],
   "source": [
    "print('Percentage of toxic comments: {}'.format(df_all['toxic'].sum() / df_all['toxic'].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize text\n",
    "import re\n",
    "pat = re.compile(u'[^a-zA-Z0-9]')\n",
    "def normalize(txt):\n",
    "    return pat.sub(' ',txt)\n",
    "    \n",
    "df_all['comment_text'] = df_all['comment_text'].apply(lambda x: normalize(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create train, test partitions\n",
    "X_all = df_all.comment_text.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, df_all.toxic.values, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create benchmark using BOW (and GradientBoostingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111699, 4873)\n",
      "(47872, 4873)\n",
      "CPU times: user 1min 38s, sys: 3.33 s, total: 1min 41s\n",
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# extract BOW as tfidf sparce matrix\n",
    "vectorizer = TfidfVectorizer(\\\n",
    "                             ngram_range=(1,3),\n",
    "                             stop_words='english',\n",
    "                             min_df=0.001,\n",
    "                             max_df=0.99,\n",
    "                             sublinear_tf=True\n",
    "                            )\n",
    "vectorizer.fit(X_all)\n",
    "X_train_csr = vectorizer.transform(X_train)\n",
    "X_test_csr = vectorizer.transform(X_test)\n",
    "print(X_train_csr.shape)\n",
    "print(X_test_csr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9548379010695187\n",
      "CPU times: user 2.58 s, sys: 206 ms, total: 2.78 s\n",
      "Wall time: 2.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# train and predict\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_csr, y_train)\n",
    "y_hat = clf.predict(X_test_csr)\n",
    "acc = accuracy_score(y_test, y_hat)\n",
    "print(\"Accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize POS tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "{'': 1, 'EX': 2, 'ADD': 3, 'RBS': 4, 'POS': 5, 'WP': 6, 'CD': 7, 'PRP': 8, 'NN': 9, 'VBZ': 10, 'RP': 11, 'JJS': 12, '$': 13, 'VBG': 14, 'FW': 15, 'RB': 16, 'XX': 17, 'SYM': 18, 'RBR': 19, 'PDT': 20, 'VB': 21, 'UH': 22, 'VBP': 23, 'IN': 24, 'NNP': 25, 'PRP$': 26, 'MD': 27, 'LS': 28, 'TO': 29, 'NNS': 30, 'CC': 31, '_SP': 32, 'JJR': 33, 'WRB': 34, 'WDT': 35, 'WP$': 36, 'VBN': 37, 'HYPH': 38, 'NNPS': 39, 'DT': 40, 'VBD': 41, '-RRB-': 42, 'JJ': 43, 'AFX': 44, '``': 45}\n",
      "CPU times: user 2min 6s, sys: 10 s, total: 2min 16s\n",
      "Wall time: 40.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create dictionary with pos tags - will use 1000 rows of data\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "SIZE=1000\n",
    "\n",
    "def doc_generator():\n",
    "    for txt in X_all[:SIZE]:\n",
    "        yield nlp(txt)\n",
    " \n",
    "doc_generator = doc_generator()\n",
    "tag_voc = dict()\n",
    "for doc in doc_generator:\n",
    "    for token in doc:\n",
    "        tag_voc[token.tag_] = 1\n",
    "\n",
    "idx = 0\n",
    "for k, v in tag_voc.items():\n",
    "    idx += 1\n",
    "    tag_voc[k] = idx\n",
    "    \n",
    "print(len(tag_voc))\n",
    "print(tag_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 20min 50s, sys: 8min 40s, total: 2h 29min 31s\n",
      "Wall time: 38min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create list of vectors for train partition\n",
    "\n",
    "def train_vec_generator():\n",
    "    for txt in X_train:\n",
    "        doc = nlp(txt)\n",
    "        yield [tag_voc.get(token.tag_,999) for token in doc]\n",
    "\n",
    "vec_generator = train_vec_generator()\n",
    "max_len = 0\n",
    "train_vec_list = list()\n",
    "for vec in vec_generator:\n",
    "    train_vec_list.append(vec)\n",
    "    vec_len = len(vec)\n",
    "    if vec_len > max_len:\n",
    "        max_len = vec_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48min 31s, sys: 2min 18s, total: 50min 49s\n",
      "Wall time: 12min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create list of vectors for test partition with proper length (pad with zeros to match comment with longest length)\n",
    "\n",
    "def test_vec_generator():\n",
    "    for txt in X_test:\n",
    "        doc = nlp(txt)\n",
    "        yield [tag_voc.get(token.tag_,999) for token in doc]\n",
    "\n",
    "vec_generator = test_vec_generator()\n",
    "test_vec_list = list()\n",
    "for vec in vec_generator:\n",
    "    test_vec_list.append(vec)\n",
    "    vec_len = len(vec)\n",
    "    if vec_len > max_len:\n",
    "        max_len = vec_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.6 s, sys: 2.47 s, total: 29 s\n",
      "Wall time: 29.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create train pos tag vector with proper length (pad with zeros to match comment with longest length)\n",
    "i=0\n",
    "for vec in train_vec_list:\n",
    "    train_vec_list[i] = [0] * (max_len - len(vec)) + vec\n",
    "    i += 1\n",
    "    \n",
    "X_train_pos_tag = np.array(train_vec_list)\n",
    "del train_vec_list\n",
    "gc.collect()\n",
    "\n",
    "# create train pos tag vector with proper length\n",
    "i=0\n",
    "for vec in test_vec_list:\n",
    "    test_vec_list[i] = [0] * (max_len - len(vec)) + vec\n",
    "    i += 1\n",
    "    \n",
    "X_test_pos_tag = np.array(test_vec_list)\n",
    "del test_vec_list\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append POS tag vector to BOW one-hot-encoded vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111699, 6958)\n",
      "(47872, 6958)\n",
      "CPU times: user 3.24 s, sys: 594 ms, total: 3.84 s\n",
      "Wall time: 3.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from scipy.sparse import hstack\n",
    "X_train_csr_2 = hstack((X_train_csr, X_train_pos_tag.reshape(X_train_pos_tag.shape[0], max_len))).tocsr()\n",
    "X_test_csr_2 = hstack((X_test_csr, X_test_pos_tag.reshape(X_test_pos_tag.shape[0], max_len))).tocsr()\n",
    "print(X_train_csr_2.shape)\n",
    "print(X_test_csr_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare AdaBoostClassifer on enhanced data against benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9504094251336899\n",
      "CPU times: user 1min 54s, sys: 3.18 s, total: 1min 57s\n",
      "Wall time: 1min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# BOW + NLP features\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=100, learning_rate=1.0)\n",
    "clf.fit(X_train_csr_2, y_train)\n",
    "y_hat = clf.predict(X_test_csr_2)\n",
    "acc = accuracy_score(y_test, y_hat)\n",
    "print(\"Accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9490307486631016\n",
      "CPU times: user 43.7 s, sys: 874 ms, total: 44.6 s\n",
      "Wall time: 44.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# BOW dataset\n",
    "\n",
    "clf.fit(X_train_csr, y_train)\n",
    "y_hat = clf.predict(X_test_csr)\n",
    "acc = accuracy_score(y_test, y_hat)\n",
    "print(\"Accuracy = {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111699, 6958)\n",
      "(47872, 6958)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_csr_2.shape)\n",
    "print(X_test_csr_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111699, 4873)\n",
      "(47872, 4873)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_csr.shape)\n",
    "print(X_test_csr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
